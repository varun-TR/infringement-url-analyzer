---
title: "assignment-1"
author: "varun"
date: "2024-08-14"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(reticulate)
use_python('/Users/trsaivarun/anaconda3/bin/python')
```

# Infringement URL Analysis üö®
This document contains a Python script designed to flatten nested data structures, extract and analyze infringing URLs, and generate insightful summaries. The project utilizes parallel processing to efficiently handle large datasets, making it suitable for high-performance environments.

## Workflow üõ†Ô∏è

**Flow Diagram:** (Add flow diagram here if available)

## ‚ú® Features

### üóÇÔ∏è Data Flattening
- Automatically flattens nested JSON or similar data structures.

### üîó Infringing URL Extraction
- Extracts infringing URLs and processes each URL individually.

### üåê Domain and IP Address Identification
- Retrieves the domain and corresponding IP address for each infringing URL.

### ‚ö° Parallel Processing
- Leverages multiple CPUs to accelerate domain and IP address extraction.

### üìä Data Summarization
- Generates key insights from the data, including the most frequently infringed domains and their associated IP addresses.

## üì• Source Data
- The nested data structure containing multiple infringing URLs, provided in a file named `response.json`.

## üì§ Output
- A flattened CSV file where each row corresponds to an infringing URL, along with its domain and IP address.

## Performance Considerations üöÄ
The script is designed to run efficiently on systems with multiple CPUs. It leverages Python's `concurrent.futures` library to parallelize the extraction of domains and IP addresses. The default configuration uses 4 CPUs, but this can be adjusted to fit the capabilities of your system.

## Code Overview

### Data Flattening

```{python}

import pandas as pd
import json
from pandas import json_normalize

# Load the JSON data
with open('/Users/trsaivarun/Downloads/response.json', 'r') as f:
    data = json.load(f)

# Normalize and flatten the JSON structure
# Normalize 'notices' list to a flat table
df = json_normalize(data['notices'], record_path=['works', 'infringing_urls'], 
                    meta=['id', 'type', 'title', 'date_sent', 'date_received', 
                          ['works', 'description']], 
                    record_prefix='infringing_')

# Rename the columns for clarity
df = df.rename(columns={
    'infringing_url': 'infringing URL',
    'works.description': 'work_description'
})

# Save the flattened data to CSV
df.to_csv('/Users/trsaivarun/Desktop/c_py:R/flattened_json_data.csv', index=False)


```
## Domain and IP Address Extraction and Parallel Processing


```{python}
import socket
from urllib.parse import urlparse
from concurrent.futures import ThreadPoolExecutor

# Function to extract domain from a URL
def extract_domain(url):
    try:
        return urlparse(url).netloc
    except Exception as e:
        return None

# Function to get IP address for a domain
def get_ip(domain):
    try:
        return socket.gethostbyname(domain)
    except socket.gaierror:
        return None

# Extract domain from infringing URLs
df['domain'] = df['infringing URL'].apply(extract_domain)

# Parallelize IP resolution using ThreadPoolExecutor
with ThreadPoolExecutor(max_workers=4) as executor:
    df['IP address'] = list(executor.map(get_ip, df['domain']))

# Save the updated DataFrame to CSV
df.to_csv('flattened_data_with_domain_ip__.csv', index=False)

```
## Data Summarization & Analysis

```{python}
df.isna().sum()
```

```{python}
import pandas as pd
import matplotlib.pyplot as plt

# Load the CSV file with the IP addresses and domains
df = pd.read_csv('flattened_data_with_domain_ip__.csv')

# Count missing values for each column
missing_counts = df.isna().sum()

# Plot bar chart
plt.figure(figsize=(10, 6))
missing_counts.plot(kind='bar', color='skyblue')
plt.xlabel('Columns')
plt.ylabel('Number of Missing Values')
plt.title('Number of Missing Values by Column')
plt.xticks(rotation=45)
plt.show()
```

```{python}
df['IP address'].fillna('not available', inplace=True) # not available since 3320 missing values
df = df.dropna(subset=['domain']) # just removing null since domains are 19 (values are small)

```


```{python}
df.isna().sum()
```



```{python}
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns


# Count the frequency of each IP address
ip_counts = df['IP address'].value_counts()

# Get the top 10 most frequent IP addresses
top_ip_addresses = ip_counts.head(10)

# Plotting
plt.figure(figsize=(12, 8))
sns.barplot(x=top_ip_addresses.values, y=top_ip_addresses.index, palette='viridis')

# Adding labels and title
plt.xlabel('Frequency')
plt.ylabel('IP Address')
plt.title('Top 10 Most Frequent IP Addresses')
plt.show()
```


```{python}
df['domain'].value_counts()
```

```{python}
# Summarization 3: Top 10 most frequent infringing URLs
df['infringing URL'].value_counts().head(10)

```


```{python}
import matplotlib.pyplot as plt

# Load data
df = pd.read_csv('flattened_data_with_domain_ip__.csv')

# Count of infringing URLs per domain
domain_counts = df['domain'].value_counts()

# Plot
plt.figure(figsize=(12, 8))
domain_counts.head(10).plot(kind='bar', color='skyblue')
plt.title('Top 10 Domains with Most Infringing URLs')
plt.xlabel('Domain')
plt.ylabel('Count of Infringing URLs')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.savefig('top_10_domains.png')
plt.show(
```


```{python}
# Most frequent infringing URLs
url_counts = df['infringing URL'].value_counts()

# Plot
plt.figure(figsize=(12, 8))
url_counts.head(10).plot(kind='bar', color='salmon')
plt.title('Top 10 Most Frequent Infringing URLs')
plt.xlabel('Infringing URL')
plt.ylabel('Count')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.savefig('top_10_infringing_urls.png')
plt.show()

```


```{python}
import pandas as pd


# Count the frequency of each IP address
ip_counts = df['IP address'].value_counts()

# Display the top 10 most frequent IP addresses
top_ip_addresses = ip_counts.head(10)

print("Top 10 Most Frequent IP Addresses:")
print(top_ip_addresses)

```


```{python}
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns


# Count the frequency of each IP address
ip_counts = df['IP address'].value_counts()

# Get the top 10 most frequent IP addresses
top_ip_addresses = ip_counts.head(10)

# Plotting
plt.figure(figsize=(12, 8))
sns.barplot(x=top_ip_addresses.values, y=top_ip_addresses.index, palette='viridis')

# Adding labels and title
plt.xlabel('Frequency')
plt.ylabel('IP Address')
plt.title('Top 10 Most Frequent IP Addresses')
plt.show()

```

```{python}
import matplotlib.pyplot as plt
import pandas as pd

# Count the occurrences of each work_description
description_counts = df['work_description'].value_counts()

# Get the top 10 most frequent work_descriptions
top_10_descriptions = description_counts.head(10)

# Set font to handle special characters
plt.rcParams['font.family'] = 'DejaVu Sans'

# Plot the top 10 results
plt.figure(figsize=(12, 8))  # Adjust the size to accommodate longer labels
top_10_descriptions.plot(kind='bar', color='skyblue')
plt.title('Top 10 Work Descriptions with the Most Fake URLs')
plt.xlabel('Work Description')
plt.ylabel('Number of Fake URLs')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()

# Show the plot
plt.show()
```

```{python}
import pandas as pd
import matplotlib.pyplot as plt

# Convert 'date_received' to datetime format if it's not already
df['date_received'] = pd.to_datetime(df['date_received'], errors='coerce')

# Count occurrences per day
daily_counts = df.groupby(df['date_received'].dt.date).size()

# Plot the daily counts
plt.figure(figsize=(14, 7))
daily_counts.plot(kind='line', marker='o', color='skyblue')
plt.title('Number of Fake URLs Received Per Day')
plt.xlabel('Date')
plt.ylabel('Number of Fake URLs')
plt.xticks(rotation=45)
plt.tight_layout()

# Show the plot
plt.show()
```

