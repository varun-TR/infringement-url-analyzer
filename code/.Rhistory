rename(
`infringing URL` = url,
work_description = description
)
# Save the flattened data to CSV
write.csv(df, '/Users/trsaivarun/Desktop/c_py:R/flattened_json_data_R.csv', row.names = FALSE)
df <- read.csv('/Users/trsaivarun/Desktop/c_py:R/flattened_json_data_R.csv')
head(df)
#using nslookup techniques for faster processing speeds.
library(dplyr)
library(urltools)
library(parallel)
# Sample DataFrame (replace this with your actual data)
df <- read.csv('/Users/trsaivarun/Desktop/c_py:R/flattened_json_data_R.csv')
# Function to extract domain from a URL
extract_domain <- function(url) {
domain(url)
}
# Function to get IP address for a domain using nslookup
get_ip <- function(domain) {
tryCatch({
result <- system(paste("nslookup", domain), intern = TRUE)
ip_lines <- grep("Address:", result, value = TRUE)
if (length(ip_lines) > 1) {
ip <- sub("Address: ", "", ip_lines[2])  # Take the second Address if multiple
} else if (length(ip_lines) == 1) {
ip <- sub("Address: ", "", ip_lines[1])
} else {
ip <- NA
}
return(ip)
}, error = function(e) {
return(NA)
})
}
# Extract domain from infringing URLs
df <- df %>%
mutate(domain = sapply(`infringing.URL`, extract_domain))
# Parallelize IP resolution using mclapply
df$IP_address <- mclapply(df$domain, get_ip, mc.cores = 4)  # Adjust the number of cores as needed
# Convert the list of IP addresses to a vector
df$IP_address <- unlist(df$IP_address)
# Save the updated DataFrame to CSV
write.csv(df, '/Users/trsaivarun/Desktop/flattened_data_with_domain_ip___R.csv', row.names = FALSE)
# some domains doesn't have urls in response.json
# Load necessary library
library(readr)
# Load the CSV file with the IP addresses and domains
df <- read_csv('/Users/trsaivarun/Desktop/flattened_data_with_domain_ip___R.csv')
# Count missing values for each column
missing_counts <- colSums(is.na(df))
# Print the result
print(missing_counts)
library(readr)
library(ggplot2)
library(dplyr)
# Load the CSV file with the IP addresses and domains
df <- read_csv('/Users/trsaivarun/Desktop/flattened_data_with_domain_ip___R.csv')
# Count missing values for each column
missing_counts <- colSums(is.na(df))
# Convert to data frame for plotting
missing_counts_df <- data.frame(
Column = names(missing_counts),
MissingValues = missing_counts
)
# Plot bar chart
ggplot(missing_counts_df, aes(x = Column, y = MissingValues)) +
geom_bar(stat = "identity", fill = "skyblue") +
labs(x = "Columns", y = "Number of Missing Values", title = "Number of Missing Values by Column") +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
theme_minimal()
df <- read_csv('/Users/trsaivarun/Desktop/flattened_data_with_domain_ip___R.csv')
# Count the frequency of each infringing URL
url_counts <- df %>%
count(`infringing.URL`) %>%
arrange(desc(n))
# Get the top 10 most frequent infringing URLs
top_urls <- head(url_counts, 10)
# Print the result
print(top_urls)
# Load necessary libraries
library(dplyr)
library(ggplot2)
library(readr)
# Load the CSV file with the IP addresses and domains
df <- read_csv('/Users/trsaivarun/Desktop/flattened_data_with_domain_ip___R.csv')
# Count the frequency of each IP address
ip_counts <- df %>%
count(`IP_address`) %>%
arrange(desc(n))
# Get the top 10 most frequent IP addresses
top_ip_addresses <- head(ip_counts, 10)
# Plotting
ggplot(top_ip_addresses, aes(x = reorder(`IP_address`, n), y = n)) +
geom_bar(stat = "identity", fill = "red") +
labs(x = 'IP Address', y = 'Frequency', title = 'Top 10 Most Frequent IP Addresses') +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for better readability
# Load necessary libraries
library(dplyr)
library(ggplot2)
library(readr)
# Load the CSV file with the IP addresses and domains
df <- read_csv('/Users/trsaivarun/Desktop/flattened_data_with_domain_ip___R.csv')
# Count of infringing URLs per domain
domain_counts <- df %>%
count(domain) %>%
arrange(desc(n))
# Get the top 10 domains with most infringing URLs
top_domains <- head(domain_counts, 10)
# Plot
ggplot(top_domains, aes(x = reorder(domain, n), y = n)) +
geom_bar(stat = "identity", fill = "skyblue") +
labs(title = 'Top 10 Domains with Most Infringing URLs',
x = 'Domain',
y = 'Count of Infringing URLs') +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# Load necessary libraries
library(dplyr)
library(ggplot2)
library(readr)
library(lubridate)  # For date manipulation
# Load the CSV file
df <- read_csv('flattened_data_with_domain_ip__.csv')
# Convert 'date_received' to Date format
df$date_received <- as.Date(df$date_received, format = "%Y-%m-%d")
# Count occurrences per day
daily_counts <- df %>%
count(date_received) %>%
arrange(date_received)
# Plot the daily counts
ggplot(daily_counts, aes(x = date_received, y = n)) +
geom_line(color = "skyblue") +
geom_point(color = "skyblue") +
labs(title = 'Number of Fake URLs Received Per Day',
x = 'Date',
y = 'Number of Fake URLs') +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for better readability
```{r}
# Load necessary libraries
library(dplyr)
library(ggplot2)
library(readr)
# Load the CSV file with the IP addresses and domains
df <- read_csv('/Users/trsaivarun/Desktop/flattened_data_with_domain_ip___R.csv')
# Count the frequency of each IP address
ip_counts <- df %>%
count(`IP_address`) %>%
arrange(desc(n))
# Get the top 10 most frequent IP addresses
top_ip_addresses <- head(ip_counts, 10)
# Plotting
ggplot(top_ip_addresses, aes(x = reorder(`IP_address`, n), y = n)) +
geom_bar(stat = "identity", fill = "red") +
labs(x = 'IP Address', y = 'Frequency', title = 'Top 10 Most Frequent IP Addresses') +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for better readability
library(jsonlite)
library(tidyr)
library(dplyr)
library(purrr)
# Load the JSON data
data <- fromJSON('/Users/trsaivarun/Downloads/response.json')
# Inspect the structure of the data
# str(data)
# Flatten the 'notices' list into a data frame
notices_df <- data$notices
# Unnest the 'works' column
notices_df <- notices_df %>%
mutate(works = map(works, ~ as.data.frame(.))) %>%
unnest(works)
# Unnest the 'infringing_urls' within 'works'
notices_df <- notices_df %>%
mutate(infringing_urls = map(infringing_urls, ~ as.data.frame(.))) %>%
unnest(infringing_urls)
# Select and rename the necessary columns
df <- notices_df %>%
select(id, type, title, date_sent, date_received, description, url) %>%
rename(
`infringing URL` = url,
work_description = description
)
# Save the flattened data to CSV
write.csv(df, '/Users/trsaivarun/Desktop/c_py:R/test-speeds/flattened_json_data_R.csv', row.names = FALSE)
df <- read.csv('/Users/trsaivarun/Desktop/c_py:R/flattened_json_data_R.csv')
head(df)
library(dplyr)
library(urltools)
library(parallel)
# Sample DataFrame (replace this with your actual data)
df <- read.csv('/Users/trsaivarun/Desktop/c_py:R/flattened_json_data_R.csv')
# Vectorized function to extract domain from a URL
df$domain <- domain(df$infringing.URL)
# Function to get IP address for a domain using nslookup
get_ip <- function(domain) {
tryCatch({
result <- system(paste("nslookup", domain), intern = TRUE)
ip_lines <- grep("Address:", result, value = TRUE)
if (length(ip_lines) > 1) {
ip <- sub("Address: ", "", ip_lines[2])  # Take the second Address if multiple
} else if (length(ip_lines) == 1) {
ip <- sub("Address: ", "", ip_lines[1])
} else {
ip <- NA
}
return(ip)
}, error = function(e) {
return(NA)
})
}
# Parallelize IP resolution using mclapply
num_cores <- detectCores() - 1  # Use all cores except one for better system responsiveness
df$IP_address <- mclapply(df$domain, get_ip, mc.cores = num_cores)
# Convert the list of IP addresses to a vector
df$IP_address <- unlist(df$IP_address)
# Save the updated DataFrame to CSV
write.csv(df, '/Users/trsaivarun/Desktop/c_py:R/test-speeds/flattened_data_with_ip_R', row.names = FALSE)
# some domains doesn't have ipaddresses in response.json -- as most of these are fake urls
# Load necessary library
library(readr)
# Load the CSV file with the IP addresses and domains
df <- read_csv('/Users/trsaivarun/Desktop/c_py:R/test-speeds/flattened_data_with_ip_R')
# Count missing values for each column
missing_counts <- colSums(is.na(df))
# Print the result
print(missing_counts)
library(dplyr)
library(urltools)
library(parallel)
# Sample DataFrame (replace this with your actual data)
df <- read.csv('/Users/trsaivarun/Desktop/c_py:R/test-speeds/flattened_json_data_R.csv')
# Vectorized function to extract domain from a URL
df$domain <- domain(df$infringing.URL)
# Function to get IP address for a domain using nslookup
get_ip <- function(domain) {
tryCatch({
result <- system(paste("nslookup", domain), intern = TRUE)
ip_lines <- grep("Address:", result, value = TRUE)
if (length(ip_lines) > 1) {
ip <- sub("Address: ", "", ip_lines[2])  # Take the second Address if multiple
} else if (length(ip_lines) == 1) {
ip <- sub("Address: ", "", ip_lines[1])
} else {
ip <- NA
}
return(ip)
}, error = function(e) {
return(NA)
})
}
# Parallelize IP resolution using mclapply
num_cores <- detectCores() - 1  # Use all cores except one for better system responsiveness
df$IP_address <- mclapply(df$domain, get_ip, mc.cores = num_cores)
library(dplyr)
library(urltools)
library(parallel)
# Sample DataFrame (replace this with your actual data)
df <- read.csv('/Users/trsaivarun/Desktop/c_py:R/test-speeds/flattened_json_data_R.csv')
# Vectorized function to extract domain from a URL
df$domain <- domain(df$infringing.URL)
# Function to get IP address for a domain using socket package
get_ip <- function(domain) {
tryCatch({
# Resolve domain to IP address using system command
ip <- system(paste("dig +short", domain), intern = TRUE)
# Only return the first IP address found
return(ifelse(length(ip) > 0, ip[1], NA))
}, error = function(e) {
return(NA)
})
}
# Parallelize IP resolution using mclapply
num_cores <- detectCores() - 1  # Use all cores except one for better system responsiveness
df$IP_address <- unlist(mclapply(df$domain, get_ip, mc.cores = num_cores))
# Save the updated DataFrame to CSV
write.csv(df, '/Users/trsaivarun/Desktop/c_py:R/test-speeds/flattened_data_with_domain_ip___R.csv', row.names = FALSE)
library(dplyr)
library(urltools)
library(parallel)
library(pingr)  # Install this package if not already installed
# Sample DataFrame (replace this with your actual data)
df <- read.csv('/Users/trsaivarun/Desktop/c_py:R/test-speeds/flattened_json_data_R.csv')
# Vectorized function to extract domain from a URL
df$domain <- domain(df$infringing.URL)
# Function to get IP address for a domain using dnslookup
get_ip <- function(domain) {
tryCatch({
ip <- dnslookup(domain)
if (length(ip) > 0) {
return(ip[1])  # Return the first IP address found
} else {
return(NA)
}
}, error = function(e) {
return(NA)
})
}
# Parallelize IP resolution using mclapply
num_cores <- detectCores() - 1  # Use all cores except one for better system responsiveness
df$IP_address <- unlist(mclapply(df$domain, get_ip, mc.cores = num_cores))
# Save the updated DataFrame to CSV
write.csv(df, '/Users/trsaivarun/Desktop/c_py:R/test-speeds/flattened_data_with_domain_ip___R.csv', row.names = FALSE)
library(dplyr)
library(urltools)
library(parallel)
library(socket)  # Install this package if not already installed
install.packages('socket')
reticulate::repl_python()
library(dplyr)
library(urltools)
# Path to your CSV file
input_csv <- '/Users/trsaivarun/Desktop/c_py:R/test-speeds/flattened_json_data_R.csv'
output_csv <- '/Users/trsaivarun/Desktop/c_py:R/test-speeds/flattened_data_with_domain_ip_R.csv'
# Call the Python script
system(paste("python3 resolve_ips.py", input_csv, output_csv))
# Load the updated CSV with IP addresses
df <- read.csv(output_csv)
reticulate::repl_python()
library(dplyr)
library(urltools)
# Path to your CSV file
input_csv <- '/Users/trsaivarun/Desktop/c_py:R/test-speeds/flattened_json_data_R.csv'
output_csv <- '/Users/trsaivarun/Desktop/c_py:R/test-speeds/flattened_data_with_domain_ip_R.csv'
# Call the Python script
system(paste("python3 resolve_ips.py", input_csv, output_csv))
# Load the updated CSV with IP addresses
df <- read.csv(output_csv)
reticulate::repl_python()
library(dplyr)
library(urltools)
# Path to your CSV file
input_csv <- '/Users/trsaivarun/Desktop/c_py:R/test-speeds/flattened_json_data_R.csv'
output_csv <- '/Users/trsaivarun/Desktop/c_py:R/test-speeds/flattened_data_with_domain_ip_R.csv'
# Construct the command to call the Python script
cmd <- paste("python3 resolve_ips.py", shQuote(input_csv), shQuote(output_csv))
# Call the Python script
system(cmd)
# Load the updated CSV with IP addresses
df <- read.csv(output_csv)
# Load necessary libraries
library(dplyr)
library(urltools)
library(curl)
# Load your data
df <- read.csv('/Users/trsaivarun/Desktop/c_py:R/test-speeds/flattened_json_data_R.csv')
# Vectorized function to extract domain from a URL
df$domain <- domain(df$infringing.URL)
# Function to get IP address for a domain using curl
get_ip <- function(domain) {
tryCatch({
ips <- nslookup(domain)
if (length(ips) > 0) {
return(ips[1])  # Return the first IP address found
} else {
return(NA)
}
}, error = function(e) {
return(NA)
})
}
# Resolve IPs in parallel using mclapply
num_cores <- parallel::detectCores() - 1  # Use all cores except one
df$IP_address <- unlist(parallel::mclapply(df$domain, get_ip, mc.cores = num_cores))
# Save the updated DataFrame to CSV
write.csv(df, '/Users/trsaivarun/Desktop/c_py:R/test-speeds/flattened_data_with_domain_ip_R.csv', row.names = FALSE)
# Display the first few rows of the updated DataFrame
head(df)
# Load necessary libraries
library(dplyr)
library(urltools)
library(curl)
# Load your data
df <- read.csv('/Users/trsaivarun/Desktop/c_py:R/test-speeds/flattened_json_data_R.csv')
# Vectorized function to extract domain from a URL
df$domain <- domain(df$infringing.URL)
# Function to get IP address for a domain using curl
get_ip <- function(domain) {
tryCatch({
ips <- nslookup(domain)
if (length(ips) > 0) {
return(ips[1])  # Return the first IP address found
} else {
return(NA)
}
}, error = function(e) {
return(NA)
})
}
# Resolve IPs in parallel using mclapply
num_cores <- parallel::detectCores() - 1  # Use all cores except one
df$IP_address <- unlist(parallel::mclapply(df$domain, get_ip, mc.cores = num_cores))
# Save the updated DataFrame to CSV
write.csv(df, '/Users/trsaivarun/Desktop/c_py:R/test-speeds/flattened_data_with_domain_ip_R.csv', row.names = FALSE)
# Display the first few rows of the updated DataFrame
head(df)
# Load necessary library
library(readr)
# Load the CSV file with the IP addresses and domains
df <- read_csv('/Users/trsaivarun/Desktop/c_py:R/test-speeds/flattened_data_with_domain_ip___R.csv')
# Count missing values for each column
missing_counts <- colSums(is.na(df))
# Print the result
print(missing_counts)
library(readr)
library(ggplot2)
library(dplyr)
# Load the CSV file with the IP addresses and domains
df <- read_csv('/Users/trsaivarun/Desktop/c_py:R/test-speeds/flattened_data_with_ip_R')
# Count missing values for each column
missing_counts <- colSums(is.na(df))
# Convert to data frame for plotting
missing_counts_df <- data.frame(
Column = names(missing_counts),
MissingValues = missing_counts
)
# Plot bar chart
ggplot(missing_counts_df, aes(x = Column, y = MissingValues)) +
geom_bar(stat = "identity", fill = "skyblue") +
labs(x = "Columns", y = "Number of Missing Values", title = "Number of Missing Values by Column") +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
theme_minimal()
library(readr)
library(ggplot2)
library(dplyr)
# Load the CSV file with the IP addresses and domains
df <- read_csv('/Users/trsaivarun/Desktop/c_py:R/test-speeds/flattened_data_with_domain_ip___R.csv')
# Count missing values for each column
missing_counts <- colSums(is.na(df))
# Convert to data frame for plotting
missing_counts_df <- data.frame(
Column = names(missing_counts),
MissingValues = missing_counts
)
# Plot bar chart
ggplot(missing_counts_df, aes(x = Column, y = MissingValues)) +
geom_bar(stat = "identity", fill = "skyblue") +
labs(x = "Columns", y = "Number of Missing Values", title = "Number of Missing Values by Column") +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
theme_minimal()
df <- read_csv('/Users/trsaivarun/Desktop/c_py:R/test-speeds/flattened_data_with_domain_ip___R.csv')
# Count the frequency of each infringing URL
url_counts <- df %>%
count(`infringing.URL`) %>%
arrange(desc(n))
# Get the top 10 most frequent infringing URLs
top_urls <- head(url_counts, 10)
# Print the result
print(top_urls)
# Load necessary libraries
library(dplyr)
library(ggplot2)
library(readr)
# Load the CSV file with the IP addresses and domains
df <- read_csv('/Users/trsaivarun/Desktop/c_py:R/test-speeds/flattened_data_with_domain_ip___R.csv')
# Count of infringing URLs per domain
domain_counts <- df %>%
count(domain) %>%
arrange(desc(n))
# Get the top 10 domains with most infringing URLs
top_domains <- head(domain_counts, 10)
# Plot
ggplot(top_domains, aes(x = reorder(domain, n), y = n)) +
geom_bar(stat = "identity", fill = "skyblue") +
labs(title = 'Top 10 Domains with Most Infringing URLs',
x = 'Domain',
y = 'Count of Infringing URLs') +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# Load necessary libraries
library(dplyr)
library(ggplot2)
library(readr)
library(lubridate)  # For date manipulation
# Load the CSV file
df <- read_csv('/Users/trsaivarun/Desktop/c_py:R/test-speeds/flattened_data_with_domain_ip___R.csv')
# Convert 'date_received' to Date format
df$date_received <- as.Date(df$date_received, format = "%Y-%m-%d")
# Count occurrences per day
daily_counts <- df %>%
count(date_received) %>%
arrange(date_received)
# Plot the daily counts
ggplot(daily_counts, aes(x = date_received, y = n)) +
geom_line(color = "skyblue") +
geom_point(color = "skyblue") +
labs(title = 'Number of Fake URLs Received Per Day',
x = 'Date',
y = 'Number of Fake URLs') +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for better readability
# Load necessary libraries
library(dplyr)
library(ggplot2)
library(readr)
# Load the CSV file with the IP addresses and domains
df <- read_csv('/Users/trsaivarun/Desktop/c_py:R/test-speeds/flattened_data_with_domain_ip___R.csv')
# Count the frequency of each IP address
ip_counts <- df %>%
count(`IP_address`) %>%
arrange(desc(n))
# Get the top 10 most frequent IP addresses
top_ip_addresses <- head(ip_counts, 10)
# Plotting
ggplot(top_ip_addresses, aes(x = reorder(`IP_address`, n), y = n)) +
geom_bar(stat = "identity", fill = "red") +
labs(x = 'IP Address', y = 'Frequency', title = 'Top 10 Most Frequent IP Addresses') +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for better readability
head(df)
reticulate::repl_python()
